---
title: "Final Report"
author: "Jack Freier and Jackson Henningfield"
date: "5/3/2020"
output: bookdown::html_document2
abstract: "This report takes an in-depth look at the many factors that influence hotel booking cancellations. Using data that includes both a series of potential explanatory variables and known observations, we build and train five models that predict the probability of a guest cancelling their hotel reservation. We then evaluate these models individually and present our findings, justifying our selection of the best model with several performance metrics as well as providing a detailed interpretation of the results."
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, messages = FALSE)
```

```{r, include=FALSE}
# Plotting and exploring
library(tidyverse)  # For plotting and summarizing
library(GGally)     # For nice scatterplot matrix 
library(ggridges)   # For joy/ridge plots
library(corrplot)   # For basic correlation matrix plot
library(naniar)     # For exploring missing values
library(pdp)        # For partial dependence plots, MARS models
library(rpart.plot) # For plotting decision trees
library(vip)        # For importance plots
library(pROC)       # For ROC curves
library(plotROC)    # For plotting ROC curves

# Making things look nice
library(lubridate)  # For nice dates
library(knitr)      # For nice tables
library(scales)     # For nice labels on graphs
library(gridExtra)  # For arranging plots
library(broom)      # For nice model output
library(janitor)    # For nice names

# Modeling
library(rsample)          # For splitting data
library(recipes)          # For keeping track of transformations
library(caret)            # For modeling
library(leaps)            # For variable selection
library(glmnet)           # For LASSO
library(earth)            # For MARS models
library(rpart)            # For decision trees
library(randomForest)     # For bagging and random forests

# Color palettes
library(RColorBrewer)

# Data
library(ISLR)
library(moderndive)
library(rattle)
library(fivethirtyeight)

library(bookdown)

theme_set(theme_minimal())
```

# Introduction

The ability to accurately assess the likelihood that a given guest cancels their reservation has important implications for the hotel industry. The goal of this paper is to derive a model that, given known values for a series of explanatory variables, can produce the probability of cancellation. We accomplish this via the use of a hotel bookings dataset, which contains just under 120,000 observations across 17 distinct variables. The supervised learning techniques that we utilize in constructing our models are based upon our response variable, `is_canceled`, a binary indicator describing the status of any given hotel booking observation. As a way of approaching this classification problem, we employ several machine learning techniques including logistic regression, LASSO, decision trees, and random forests. Before attempting any of these approaches, we first partition the data into two sets of observations, one for training the models and one for evaluating them. The plot below shows the distribution of the response variable, `is_canceled`, for both of these training and testing subsets. It should be noted that both have extremely similar distributions of the response variable, with about 37% of the observations being cancellations (indicated by an `is_canceled` value of one). Furthermore, this suggests that any model we produce should have an accuracy greater than 63%, as any accuracy lower than this would be outperformed by a model that simply predicts an `is_canceled` value of zero for every observation.

```{r, include=FALSE}
# Reading in the hotel booking demand data
hotel_demand <- read.csv("hotel_bookings.csv",stringsAsFactors = FALSE)

# Change variable classes
hotel_numeric <- hotel_demand %>%
    select(-hotel,-arrival_date_month, -meal, -country, -market_segment, -distribution_channel, 
           -reserved_room_type, -reserved_room_type, -assigned_room_type, -deposit_type, -agent, 
           -company, -customer_type, -reservation_status_date, -reservation_status, -children
           )

hotel_category <- hotel_demand %>% 
  select(-lead_time, -arrival_date_year, -arrival_date_week_number, -arrival_date_day_of_month, 
         -stays_in_weekend_nights, -stays_in_week_nights, -adults, -children, -babies, 
         -previous_bookings_not_canceled, -booking_changes, -days_in_waiting_list, -adr, 
         -required_car_parking_spaces, -total_of_special_requests
         ) %>% 
  mutate(is_canceled = as.factor(is_canceled)) %>% 
  mutate(is_repeated_guest = as.factor(is_repeated_guest))

hotel_demand <- hotel_demand %>%
  mutate(
    is_canceled = as.factor(is_canceled),
    is_repeated_guest = as.factor(is_repeated_guest),
    assigned_correct_room = as.factor(ifelse(as.character(assigned_room_type) == as.character(reserved_room_type), 
                                             1, 0)),
    has_previous_cancellations = ifelse(previous_cancellations > 0, 1, 0)
  ) %>%
  select(
    -arrival_date_year, -arrival_date_month, -arrival_date_day_of_month, -stays_in_week_nights,
    -previous_bookings_not_canceled, -market_segment, -agent, -company, -reservation_status_date, -assigned_room_type, 
    -reserved_room_type, -previous_cancellations, -required_car_parking_spaces, -deposit_type, -reservation_status, 
    -distribution_channel, -children
  )

# Putting all countries into regions based on ITU Region classification
names(hotel_demand)[names(hotel_demand) == "country"] <- "region"

attach(hotel_demand)
hotel_demand$region[region %in% c("ABW","AIA","ARG","BHS","BOL","BRA","BRB","CHL","COL","CRI","CUB","CYM",
                                "DMA","DOM","ECU","GLP","GTM","GUY","HND","JAM","KNA","LCA","MEX","NIC","PAN",
                                "PER","PRI","PRY","SLV","SUR","URY","USA","VEN","VGB")] <- "Americas"
hotel_demand$region[region %in% c("ALB","AND","AUT","BEL","BGR","BIH","CHE","CYP","CZE","DEU","DNK","ESP",
                                "EST","FIN","FRA","FRO","GBR","GEO","GGY","GIB","GRC","HRV","HUN","IMN","IRL",
                                "ISL","ISR","ITA","JER","LIE","LTU","LUX","LVA","MCO","MKD","MLT","MNE","NLD",
                                "NOR","POL","PRT","ROU","SMR","SRB","SVK","SVN","SWE","TUR","UKR","JEY")] <- "Europe"
hotel_demand$region[region %in% c("ASM","ATF","AUS","BGD","CHN","CN","FJI","HKG","IDN","IND","IRN","JPN",
                                "KHM","KIR","KOR","LAO","LKA","MAC","MDV","MMR","MYS","NAM","NCL","NPL","NZL",
                                "PAK","PHL","PLW","PYF","SGP","THA","TMP","TWN","UMI","VNM")] <- "Asia & Pacific"
hotel_demand$region[region %in% c("AGO","BDI","BEN","BFA","BWA","CAF","CIV","CMR","CPV","ETH","GAB","GHA","GNB",
                                "KEN","KWT","MDG","MLI","MOZ","MUS","MWI","MYT","NGA","RWA","SEN","SLE","STP",
                                "SYC","TGO","TZA","UGA","ZAF","ZMB","ZWE")] <- "Africa"
hotel_demand$region[region %in% c("ARE","BHR","COM","DJI","DZA","EGY","IRQ","JOR","LBN","LBY","MAR","MRT",
                                  "OMN","QAT","SAU","SDN","SYR","TUN")] <- "Arab States"
hotel_demand$region[region %in% c("ARM","ATA","AZE","BLR","KAZ","NULL","RUS","TJK","UZB")] <- "CIS"

# Divide into training and testing
set.seed(253)
hotel_split <- initial_split(hotel_demand, prop = 0.5, 
                             strata = is_canceled)
hotel_train <- training(hotel_split)
hotel_test <- testing(hotel_split)

# Distribution of respone for the training data
table(hotel_train$is_canceled) %>% prop.table()

# Distribution of respone for testing data
table(hotel_test$is_canceled) %>% prop.table()
```

```{r, echo = FALSE, fig.cap="Proportion of the `is_canceled` response variable for both the training and testing datasets. The exact proportion of cancelled hotel bookings for both subsets is 0.3704."}
split_dist <- tibble(
  Training = hotel_train$is_canceled,
  Testing = hotel_test$is_canceled
)

split_dist <- pivot_longer(data = split_dist, cols = c("Training", "Testing"), names_to = "Set")

split_dist %>%
  ggplot(aes(x = Set, fill = is_canceled)) +
  geom_bar(position = "fill") +
  labs(x = "Subset", y = "Proportion", fill = "Is Canceled") +
  ggtitle("Proportion of the Response for Training and Testing Datasets")
```

# Exploratory Work 

In order to get a broad sense of how each variable influences our response, we first create exploratory plots for hotel cancellations. These plots are split into two categories, one comprising numeric predictors and the other categorical predictors. Each of these visualizations gives insight into how the particular variable of interest proportionally influences cancellations.

```{r, echo = FALSE, fig.width=12, fig.height=9, fig.cap="Example exploratory plots for four variables of interest (two numeric, two categorical). (Upper left) The median lead time for cancelled bookings is substantially higher than non-cancelled bookings. (Upper right) There are proportionally more cancellations from week 14 to week 44. (Lower left) There are proportionally more cancellations for bookings where the guest was assigned the correct room. (Lower right) There are proportionally more cancellations for guests who have previously cancelled a hotel booking."}

# Numeric
explanatory_1 <- hotel_train %>%
  ggplot(aes(x = is_canceled, y = lead_time)) +
  geom_boxplot() +
  labs(x = "Is Canceled", y = "Lead Time")

explanatory_2 <- hotel_train %>%
  ggplot(aes(x = arrival_date_week_number, fill = is_canceled, color = is_canceled)) +
  geom_density(alpha = 0.6) +
  labs(x = "Arrival Date Week Number", y = "Density", fill = "Is Canceled") +
  guides(color = FALSE) +
  scale_x_continuous(breaks = seq(0, 52, 4), limits = c(0, 53))


# Categorical
explanatory_3 <- hotel_train %>% 
  ggplot(aes(x = factor(assigned_correct_room), fill = as.factor(is_canceled))) +
  geom_bar(position = "fill") +
  labs(x = "Assigned Correct Room", y = "Proportion", fill = "Is Canceled")

explanatory_4 <- hotel_train %>% 
  ggplot(aes(x = factor(has_previous_cancellations), fill = as.factor(is_canceled))) +
  geom_bar(position = "fill") +
  labs(x = "Has Previous Cancellations", y = "Proportion", fill = "Is Canceled")

grid.arrange(explanatory_1, explanatory_2, explanatory_3, explanatory_4, ncol = 2)
```

As a way of narrowing our search process, we also construct a correlation plot for the numeric predictors. This figure is intended to provide a sense of how highly correlated each variable is with one another, helping us to reduce the pool of potential predictors while safeguarding against multicollinearity. After identifying pairs of variables that are highly correlated, we opt to remove one of the two from the dataset. Seen in the plot below, the specific pair of predictors that demonstrated the largest correlation value is `previous_bookings_not_canceled` and `is_repeated_guest`, with the former being excluded from the subsequent modeling process.

```{r, include=FALSE}
# Making correlation plot for numeric subset
H <- cor(hotel_numeric)
head(round(H,2))
```


```{r, echo = FALSE, fig.width=12, fig.height=9, fig.cap="Plot detailing the correlation between individual numeric variables included in the dataset. The opacity of the square signifies the magnitude, with blue representing positive correlation and red representing negative correlation."}

# Numeric var. correlation plot
corrplot(H, method = "color")
```

Lastly, before beginning the modeling process, we manipulate or remove several problematic variables included in the dataset. These variables are characterized by a series of issues, such as extreme crowding of particular categorical levels or numeric values (while other levels or values are very sparsely populated). To account for these deficiencies, we create new variables that aggregate information stored in the previous variables' levels (i.e. turning the `country` variable into a `region` variable based on geographic boundaries).


# Modeling Process

Our goal is to create the "best" possible model that is fit for predicting whether or not a particular reservation is canceled or not. We define the "best" possible model as that which has the highest testing accuracy, since we can measure a model's predictive power based on its performance on the testing dataset.

In our study, we created five models with `is_canceled` as our response variable. These five models (logistic regression with seven variables, logistic regression with all variables, LASSO, a classifcation tree, and a random forest) and their outputs can be seen below. We trained each model on the training set and calculated the training and cross-validated accuracies as well as the areas under each ROC curve. The code chunks below give a run down of all of these metrics, all of which assisted us in our decision to isolate one model as the best one. 

Our first model output here is a logistic regression that included our seven most important variables as shown in the random forest variable importance plot (shown later in this report). This model was created in order to ease the interpretation; since this model only has 7 variables compared to the full logistic model, with 14 variables, we thought that this would be a good example of how the number of predictors in a model affects its overall performance.

```{r, include=FALSE}
# Logistic Regression (top 7 variables)
set.seed(253)

hotel_logistic_best <- train(
    is_canceled ~ lead_time + total_of_special_requests + adr + assigned_correct_room + booking_changes + 
      arrival_date_week_number + has_previous_cancellations,
    data = hotel_train,
    method = "glm",
    family = "binomial",
    trControl = trainControl(method = "cv", number = 5),
    metric = "Accuracy",
    na.action = na.omit
)

# Model summary
summary(hotel_logistic_best)

# Cross-validated accuracy
hotel_logistic_best$results %>%
  select(Accuracy)

# Confusion matrix
confusionMatrix(data = predict(hotel_logistic_best, type = "raw"),
                reference = hotel_train$is_canceled,
                positive = "1") 
```

```{r, echo=FALSE}
# ROC curve and AUC for seven var. logistic
preds_logistic_best <- hotel_train %>% 
  mutate(pred_prob = predict(hotel_logistic_best$finalModel, 
                             type = "response"))

preds_logistic_best %>% 
  ggplot(aes(d = as.numeric(is_canceled) - 1, m = pred_prob)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray") +
  labs(x = "False Positive Fraction", y = "True Positive Fraction")

preds_logistic_best %>%
  roc(is_canceled ~ pred_prob, data = .) %>% 
  auc()
```

Our second model is the full logistic regression model with all of our predictors. As seen in the confusion matrix and the ROC plot, this model was one of our poorest performing models in terms of the testing accuracy and the testing AUC. The final results table at the end of the paper confirms this as well.

```{r, include=FALSE}
# Logistic Regression (all variables)
set.seed(253)

hotel_logistic <- train(
    is_canceled ~ .,
    data = hotel_train,
    method = "glm",
    family = "binomial",
    trControl = trainControl(method = "cv", number = 5),
    metric = "Accuracy",
    na.action = na.omit
)

# Model summary
summary(hotel_logistic)

# Cross-validated accuracy
hotel_logistic$results %>%
  select(Accuracy)

# Confusion matrix
confusionMatrix(data = predict(hotel_logistic, type = "raw"),
                reference = hotel_train$is_canceled,
                positive = "1") 
```

```{r, echo = FALSE}
# ROC curve and AUC
preds_logistic <- hotel_train %>% 
  mutate(pred_prob = predict(hotel_logistic$finalModel, 
                             type = "response"))

preds_logistic %>% 
  ggplot(aes(d = as.numeric(is_canceled) - 1, m = pred_prob)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray") +
  labs(x = "False Positive Fraction", y = "True Positive Fraction")

preds_logistic %>%
  roc(is_canceled ~ pred_prob, data = .) %>% 
  auc()
```

Our third model was a LASSO model where we used all of the predictors in our training set. It had similar results to our full logistic regression model. As seen in the lambda vs. accuracy graph, the accuracy quickly drops off after a cp of .01.

```{r, include=FALSE}
# LASSO
set.seed(253)

lambda_grid <- 10^seq(-4, -1, length = 100)

hotel_lasso <- train(
    is_canceled ~ .,
    data = hotel_train,
    method = "glmnet",
    family = "binomial",
    trControl = trainControl(method = "cv", number = 5),
    tuneGrid = data.frame(alpha = 1, 
                          lambda = lambda_grid),
    metric = "Accuracy",
    na.action = na.omit
)
```

```{r, echo=FALSE}
# Plot of lambda vs accuracy
hotel_lasso$results %>% 
  ggplot(aes(x = lambda, y = Accuracy)) +
  geom_line(size = 1.05) +
  scale_x_log10() +
  labs(x = "Lambda", y = "Accuracy")
```

```{r, include = FALSE}
# Best value of lambda
hotel_lasso$bestTune$lambda

# Accuracy associated with this lambda value
hotel_lasso$results %>%
  filter(Accuracy == max(Accuracy)) %>%
  select(lambda, Accuracy)

# Confusion matrix
confusionMatrix(data = predict(hotel_lasso, type = "raw"),
                reference = hotel_train$is_canceled,
                positive = "1")
```

```{r, echo=FALSE}
# ROC curve and AUC
preds_lasso <- hotel_train %>% 
  mutate(pred_prob = predict(hotel_lasso, 
                             type = "prob")$`1`)

preds_lasso %>% 
  ggplot(aes(d = as.numeric(is_canceled) - 1, m = pred_prob)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray") +
  labs(x = "False Positive Fraction", y = "True Positive Fraction")

preds_lasso %>%
  roc(is_canceled ~ pred_prob, data = .) %>% 
  auc()
```

The fourth model was a classfication tree; this model was our second best model in terms of performance and its accuracy. The cp vs. accuracy plot tells us that the accuracy of our model dips rather swiftly past the best tune cp of 0.0001456348. Furthermore, the ROC curve is bowed out in a way that shows that the true positive rate is greater than the false positive rate at the certain cp points on the graph such as 0.44, 0.35, and 0.26 and so forth.

```{r, include=FALSE}
# Classification Tree
set.seed(253)

hotel_tree <- train(
  is_canceled ~ .,
  data = hotel_train,
  method = "rpart",
  tuneGrid = data.frame(cp = 10^seq(-4, -2 , length = 50)),
  trControl = trainControl(method = "cv", number = 5),
  metric = "Accuracy",
  na.action = na.omit
)
```

```{r, echo=FALSE}
# Plot of cp vs accuracy
hotel_tree$results %>%
  ggplot(aes(x = cp, y = Accuracy)) +
  geom_line(size = 1.05) +
  labs(x = "cp", y = "Accuracy")
```

```{r, include=FALSE}
# Best cp value
hotel_tree$bestTune$cp

# Accuracy associated with this cp value
hotel_tree$results %>%
  filter(Accuracy == max(Accuracy)) %>%
  select(cp, Accuracy)

# Confusion matrix
confusionMatrix(data = predict(hotel_tree, type = "raw"),
                reference = hotel_train$is_canceled,
                positive = "1")
```

```{r, echo=FALSE}
# ROC curve and AUC
preds_tree <- hotel_train %>% 
  mutate(pred_prob = predict(hotel_tree, 
                             type = "prob")$`1`)

preds_tree %>% 
  ggplot(aes(d = as.numeric(is_canceled) - 1, m = pred_prob)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray") +
  labs(x = "False Positive Fraction", y = "True Positive Fraction")

preds_tree %>%
  roc(is_canceled ~ pred_prob, data = .) %>% 
  auc()
```

The fifth and final model is our best performing one: the random forest. This model has the highest accuracy and the most bowed out ROC curve which is another signal of the high perforamce capability. The OOB error vs. the number of trees graph tells us that the error flattens out at about forty trees. Overall, this error is rather low (less than 20%), and we know this model isn't overfitted on the training set since its testing accuracy is also fairly high (see Results and Next Steps).

```{r, include=FALSE, cache=TRUE}
# Random Forest
set.seed(253)

hotel_randf_oob <- train(
  is_canceled ~ .,
  data = hotel_train, 
  method = "rf",
  trControl = trainControl(method = "oob"),
  tuneGrid = data.frame(mtry = c(4,6,8)),
  ntree = 75,
  importance = TRUE,
  nodesize = 5,
  na.action = na.omit
)

# Accuracy associated with the best model
hotel_randf_oob$results %>%
  filter(Accuracy == max(Accuracy)) %>%
  select(Accuracy)
```

```{r, echo=FALSE}
# Plot of number of trees vs OOB error
plot(hotel_randf_oob$finalModel, main = "OOB Error by Number of Trees (Where Error = 1 - Accuracy)")


# Confusion matrix
confusionMatrix(data = predict(hotel_randf_oob, type = "raw"),
                reference = hotel_train$is_canceled,
                positive = "1")

# ROC Curve and AUC
preds_randf <- hotel_train %>% 
  mutate(pred_prob = predict(hotel_randf_oob, 
                             type = "prob")$`1`)

preds_randf %>% 
  ggplot(aes(d = as.numeric(is_canceled) - 1, m = pred_prob)) + 
  geom_roc(labelround = 2, size = 1,
           linealpha = .5, pointalpha = .8) +
  geom_abline(slope = 1, intercept = 0, color = "gray") +
  labs(x = "False Positive Fraction", y = "True Positive Fraction")

preds_randf %>%
  roc(is_canceled ~ pred_prob, data = .) %>% 
  auc()
```

```{r, fig.cap="Variable importance plot for our random forest model. Illustrates the relative importance of each variable utilized in the random forest. The two variables that stand out the most are `lead_time` and `total_of_special_requests`, with each being dramatically more important than the subsequent four variables."}
# VIP Plot
vip(hotel_randf_oob$finalModel, num_features = 31, bar = FALSE) + ggtitle("Random Forest - Variable Importance Plot")
```

This variable importance plot shows the rankings of each variable's importance for our random forest model. For example, our top-ranked variable is lead_time which coincides with the smallest mean decrese in node impurity (i.e. the highest Gini Index value). So, lead_time creates the most heterogeneous nodes meaning this variable causes the tree to split earlier; it is one of the first branches.


```{r, fig.cap="Partial dependence plot for the `lead_time` variable. Larger lead times tend to decrease the probability of a cancellation whereas shorter lead times tend to increase it."}
# Partial Dependence Plots
partial(hotel_randf_oob, pred.var = "lead_time", 
        grid.resolution = 50) %>% 
  autoplot() + 
  ggtitle("Random Forest - Partial Dependence Plot") +
  xlab("Lead Time")
```

As aforementioned, the predictor, `lead_time`, is the most important variable in our random forest model per the variable importance plot above. The partial dependence plot tells us the effect that lead_time has on the probability of a reservation being canceled, with all other variables being held at their average level. For small values of lead time, the y_hat value is close to one, signaling that these quickly-made reservations are far more likely to be canceled. The probability of cancelation quickly tapers off as lead time increases, meaning that reservations made far in advance will most likely be kept, holding all other predictors constant.

# Results and Next Steps


| Model             | Training Acc.| CV Acc.    |   AUC  | Test Acc.| Test ACU|
|-------------------|--------------|------------|--------|----------|---------|
|hotel_logistic     | 0.7785       |  0.7779    | 0.8191 |  0.7779  | 0.7325  |
|hotel_lasso        | 0.7785       |  0.7781    | 0.8190 |  0.7778  | 0.7325  |
|hotel_tree         | 0.8343       |  0.8114    | 0.8796 |  0.8128  | 0.7681  |
|hotel_randf_oob    | 0.8358       |  0.8357    | 0.8984 |  0.8375  | 0.7859  |
|hotel_logistic_best| 0.7355       | 	0.7353    | 0.7931 |  0.7363  | 0.7159  |


After completing training and testing for our five models and analysing their summaries, confusion matricies, and ROC plots, we came to the conclusion that the random forest is our best model; it not only performs the best on the training dataset, but also performs exceptionally well on the portion of the data reserved for testing. In terms of interpretaion, this tree is structually complex due to the number of variables that we allow for; however, combining the natural intuition of trees along with the information that variable importance plots and partial dependence plots give us, we are able to provide an intuitive representation of our hotel booking situation.

As for future extentions of our work, the idea of clustering caught our eye as a possible next step to further hone our model's predictive performance. Because hotel bookings dataset is quite large (over 100,000 observations), it is a good candidate for additional inquiries. Using non-supervised machine learning techniques, like hierarchical or k-means clustering, would allow us to divide this dataset into like groups, which could in turn assist in making models that focus on certain elements of a hotel reservation.

